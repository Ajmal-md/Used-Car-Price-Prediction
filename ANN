# ANN
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# ----------- Load Dataset -----------
# Load the dataset (update the path with your dataset location)
data = pd.read_csv("/content/encoded_dataset.csv")

# ----------- Define Target Column -----------
# Define the target column
target_column = "sale_price"

# ----------- Split Features and Target -----------
# Separate features and target variable
X = data.drop(columns=[target_column])  # Features
y = data[target_column]  # Target variable

# ----------- Split Dataset into Training and Testing Sets -----------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ----------- Scale Numerical Data -----------
# Standardize the features for ANN
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ----------- Define the Neural Network Model -----------
# Build the Sequential Neural Network
model = Sequential([
    Dense(128, activation='relu', input_dim=X_train_scaled.shape[1]),  # Input Layer + Hidden Layer 1
    Dropout(0.3),  # Dropout for regularization
    Dense(64, activation='relu'),  # Hidden Layer 2
    Dropout(0.3),  # Dropout for regularization
    Dense(1, activation='linear')  # Output Layer for regression
])

# ----------- Compile the Model -----------
# Compile the ANN model with Adam optimizer and MSE loss
model.compile(optimizer='adam', loss='mse', metrics=['mse'])

# ----------- Early Stopping Callback -----------
# Stop training if validation loss doesn't improve for 10 consecutive epochs
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True  # Restore best model after stopping
)

# ----------- Train the Neural Network -----------
# Fit the ANN model and save training history for plotting
history = model.fit(
    X_train_scaled, y_train,
    validation_split=0.2,  # Use 20% of training data for validation
    epochs=100,  # Increase epochs for better learning
    batch_size=32,  # Batch size for training
    callbacks=[early_stopping],
    verbose=1  # Show training progress
)

# ----------- Predict on Test Data -----------
# Make predictions on the test data
y_pred = model.predict(X_test_scaled).flatten()

# ----------- Evaluate Model Performance -----------
# Calculate Evaluation Metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)  # Root Mean Squared Error
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)  # R-squared score

# Print Model Performance Metrics
print(f"âœ… ANN RMSE: {rmse:.2f}")
print(f"âœ… ANN MAE: {mae:.2f}")
print(f"âœ… R2 Score: {r2:.2f}")

# ----------- Display Sample Predictions -----------
# Show first 5 sample predictions
print("\nðŸŽ¯ Sample Predictions:")
print(f"Actual: {y_test.values[:5]}")
print(f"Predicted: {y_pred[:5]}")

# ----------- Plot Training and Validation Loss -----------
# Plot the training and validation loss to visualize model performance
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss', color='teal')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.title('Training and Validation Loss Curve')
plt.legend()
plt.show()

# ----------- Plot Actual vs Predicted Values -----------
# Plot a scatter plot to compare actual and predicted values
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, color='dodgerblue', label='Predicted vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.xlabel('Actual Sale Price')
plt.ylabel('Predicted Sale Price')
plt.title('Actual vs Predicted Sale Price')
plt.legend()
plt.show()
